{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, I used Switchboard Dialog Act Corpus (SwDA) to develop a dialog act classifier that can differentiate three types of dialog act -- **i.e., starting conversation, closing conversation, and info request** -- from all other types of dialog act. Since these three of types of dialog act are the most common types of dialog act people interact with a machine conversation system, the ability to correctly notice and differentiate those dialog acts is crucial for a successful machine conversation system. This is the basic reasoning why we want to develop a classifier which aims to differentiate those three types of dialog act.\n",
    "\n",
    "In the following discussion, I first explained how I obtained and processed corpus data, then explained how to specify the model, lastly discussed model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the corpus I used was Switchboard Dialog Act corpus. However, I didn't use the original dataset. A  reformatted version of SwDA corpus, produced by Sanjay Meena, was used instead [1]. This version of SwDA corpus provided us conversational transcripts did not contain any NLP notation or marker, only consisted by plain English text. It is much more easier for us to transform plain English text to language feature than working with NLP notation, as we can see at below.\n",
    "\n",
    "In fact, since the only language feature I planned use was word vectors, which can easily generated by applying pre-trained word2vec network to corpus, the preprocessing steps I needed were the following: tokenized conversational transcripts, and made all conversational part have the same length. It was done by padding, i.e., adding empty word to short conversational parts until they all have the same length as the longest conversational part in the corpus.\n",
    "\n",
    "After finished tokenization and padding, we paired conversational parts with their correspondent dialog act tag -- one of starting conversation, info request, closing conversation, and **other dialog act**, labeled as 0,1,2,3 respectively (see [2] for tag type). The proportion of training data, validation data, testing data were 72%, 8%, 20%, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def recode_tag(tag):\n",
    "    \"\"\"\n",
    "    recode tag\n",
    "    0 = greeting (Conventional-opening)\n",
    "    1 = info request\n",
    "    2 = goodbye (Conventional-closing)\n",
    "    3 = other tags\n",
    "    \"\"\"\n",
    "    if re.search('(qy|qw|qo|qr|qrr|\\^d|\\^g)', tag):\n",
    "        return 1 \n",
    "    elif re.search('fc',tag):\n",
    "        return 0\n",
    "    elif re.search('fp',tag):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "corpus = pd.read_csv('switchboard_complete.csv', usecols =['caller','clean_text','act_tag','act_label_1'])\n",
    "corpus = corpus.dropna().reset_index()\n",
    "\n",
    "DICT_SIZE = 20000\n",
    "MAX_TEXT_LENGTH = 80\n",
    "\n",
    "tokenizer = Tokenizer(num_words= DICT_SIZE)\n",
    "tokenizer.fit_on_texts(corpus['clean_text'])\n",
    "corpus['text'] = tokenizer.texts_to_sequences(corpus['clean_text'])\n",
    "corpus['text'] = [arr for arr in sequence.pad_sequences(corpus['text'], maxlen= MAX_TEXT_LENGTH)]   \n",
    "corpus['tag'] = corpus['act_tag'].apply(recode_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "all_idx = set(list(range(len(corpus))))\n",
    "train_idx=random.sample(all_idx, k=int(np.ceil(len(corpus)*0.8)))\n",
    "test_idx = all_idx.difference(set(train_idx))\n",
    "\n",
    "val_idx = random.sample(train_idx, k=int(np.ceil(len(train_idx)*0.1)))\n",
    "train_idx = set(train_idx).difference(set(val_idx))\n",
    "\n",
    "def split_xy(data, idx):\n",
    "    x = np.stack(data.loc[idx, 'text'].values,axis=0) \n",
    "    y = OneHotEncoder().fit_transform(np.reshape(data.loc[idx, 'tag'].values,[-1,1])).toarray()\n",
    "    return x,y\n",
    "    \n",
    "train_x, train_y = split_xy(corpus, train_idx)\n",
    "val_x, val_y = split_xy(corpus, val_idx)\n",
    "test_x, test_y = split_xy(corpus, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since every conversational part has different length, it's natural to utilize a sequence model to work with this kind of training data. Here I specify a LSTM neural network with 128 hidden units as my model. The input of LSTM network were word vectors, which generated from feeding words in conversational part into the word2vec network. The length of word vectors were fixed at 32. The output of LSTM network were probabilities of the conversational part belonged to one of the four dialog act classes. The other training parameters were assigned with commonly used value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "def build_model(dict_size, embedding_size, text_length, num_class):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(dict_size, embedding_size, input_length=text_length))\n",
    "    model.add(LSTM(128, return_sequences =False))\n",
    "    model.add(Dense(num_class, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_params = {\n",
    "    # model spec\n",
    "    'build_fn': build_model,\n",
    "    'dict_size': DICT_SIZE,\n",
    "    'embedding_size': 32,\n",
    "    'text_length': MAX_TEXT_LENGTH,\n",
    "    'num_class':4,    \n",
    "    # training spec \n",
    "    'epochs': 3,\n",
    "    'batch_size': 64,\n",
    "    'verbose': 1,\n",
    "    'validation_data': (val_x, val_y),\n",
    "    'shuffle': True\n",
    "}\n",
    "\n",
    "mdl = KerasClassifier(**model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cells below, we can see that the LSTM classifier had 98.65% accuracy on training data, 98.55% accuracy on validation data, and 98.51% accuracy on testing data. Since people use very specific words and phrases to develop these three kind of dialog act, for example, \"hello\" for opening conversation, \"see you\" for closing conversation, and using \"What\",\"Why\",\"Where\" for info requests, it's not so surprising that a trivially tuned model can achieve such a high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 155905 samples, validate on 17323 samples\n",
      "Epoch 1/3\n",
      "155905/155905 [==============================] - 460s 3ms/step - loss: 0.0723 - acc: 0.9791 - val_loss: 0.0572 - val_acc: 0.9832\n",
      "Epoch 2/3\n",
      "155905/155905 [==============================] - 486s 3ms/step - loss: 0.0524 - acc: 0.9846 - val_loss: 0.0550 - val_acc: 0.9847\n",
      "Epoch 3/3\n",
      "155905/155905 [==============================] - 512s 3ms/step - loss: 0.0460 - acc: 0.9865 - val_loss: 0.0551 - val_acc: 0.9855\n"
     ]
    }
   ],
   "source": [
    "hist = mdl.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43306/43306 [==============================] - ETA:  - 29s 678us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9851694914978966"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://sanjaymeena.io/tech/nlp/Simplified-Switchboard-Corpus/\n",
    "[2] https://web.stanford.edu/~jurafsky/ws97/manual.august1.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
